import streamlit as st 
 
# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ 
if 'logged_in' not in st.session_state: 
    st.session_state.logged_in  = False 
    st.session_state.username  = None 
 
def check_login(username, password):
    """ä»secrets.toml éªŒè¯ç”¨æˆ·"""
    valid_users = st.secrets.users  
    return username in valid_users and valid_users[username] == password 
 
def login_page():
    """ç™»å½•ç•Œé¢"""
    st.title("LOGIN")
    with st.form("login_form"): 
        username = st.text_input(" ç”¨æˆ·å")
        password = st.text_input(" å¯†ç ", type="password")
        if st.form_submit_button(" ç™»å½•"):
            if check_login(username, password):
                st.session_state.logged_in  = True 
                st.session_state.username  = username 
                st.rerun()   # åˆ·æ–°é¡µé¢è·³è½¬ 
            else:
                st.error(" è®¤è¯å¤±è´¥")
 
def main_page():
    """ä¸»åŠŸèƒ½ç•Œé¢"""
    # è®¾ç½®é¡µé¢é…ç½®
    st.set_page_config(page_title="ç¤¾äº¤ç½‘ç»œèˆ†æƒ…åˆ†æç³»ç»Ÿ", page_icon=":guardsman:", layout="wide")

    # æ˜¾ç¤ºèƒŒæ™¯å›¾ç‰‡
    st.image("assets/your_name.jpg", use_container_width=True)

    # é¡µé¢æ ‡é¢˜
    st.title("ç¤¾äº¤ç½‘ç»œèˆ†æƒ…åˆ†æç³»ç»Ÿ")

    # ç³»ç»Ÿä»‹ç»
    st.markdown("""
    æ¬¢è¿ä½¿ç”¨ **ç¤¾äº¤ç½‘ç»œèˆ†æƒ…åˆ†æç³»ç»Ÿ**ã€‚è¯¥ç³»ç»Ÿå¯ä»¥å¸®åŠ©æ‚¨ï¼š
    - ä¸Šä¼ ç¤¾äº¤ç½‘ç»œæ•°æ®æ–‡ä»¶ï¼ˆå¦‚ CSV æ ¼å¼ï¼‰
    - æŸ¥çœ‹æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†
    - ç”Ÿæˆè¯äº‘ï¼Œå±•ç¤ºæ–‡æœ¬æ•°æ®çš„å…³é”®è¯
    - è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œäº†è§£æ•°æ®ä¸­çš„æƒ…æ„Ÿå€¾å‘
    - ä½¿ç”¨ä¸»é¢˜å»ºæ¨¡ï¼ˆLDA æˆ– KMeansï¼‰å¯¹æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æ
    - æŸ¥çœ‹èˆ†æƒ…åœ°åŸŸåˆ†å¸ƒ
    
    è¯·é€‰æ‹©å·¦ä¾§èœå•ä¸­çš„åŠŸèƒ½ï¼Œå¼€å§‹æ‚¨çš„åˆ†æä¹‹æ—…ï¼
""")
def main(): 
    # ä¸»ç¨‹åºé€»è¾‘ 
    if not st.session_state.logged_in: 
        login_page()
        st.stop()   # é˜»æ­¢æœªç™»å½•è®¿é—® 
    else:
        main_page()

if __name__ == "__main__":
    main()       

import streamlit as st
import pandas as pd
import datetime

# æƒé™éªŒè¯ï¼ˆæ‰€æœ‰å­é¡µé¢éƒ½éœ€è¦æ·»åŠ ï¼‰
if not st.session_state.get('logged_in'):
    st.error("è¯·å…ˆç™»å½•")
    st.stop()   # é˜»æ­¢ç»§ç»­åŠ è½½

def handle_file_upload(file):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶å¹¶ä¿å­˜åˆ° session_state """
    try:
        data = pd.read_csv(file)
        st.session_state.data = data
        return data
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{e}")
        st.stop()

# ä¸Šä¼  CSV æ–‡ä»¶
uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªCSVæ–‡ä»¶(è¯·ç¡®å®šå­˜åœ¨reviewåˆ—å’Œipåˆ—)", type=["csv"])

if uploaded_file is not None:
    # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    data = handle_file_upload(uploaded_file)
    st.success("æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
    # ä¿å­˜ä¸Šä¼ å†å²
    if 'upload_history' not in st.session_state:
        st.session_state.upload_history = []
    # ä¿å­˜æ–‡ä»¶åå’Œä¸Šä¼ æ—¶é—´
    file_info = {
        "æ–‡ä»¶å": uploaded_file.name,
        "ä¸Šä¼ æ—¶é—´": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    st.session_state.upload_history.insert(0, file_info)  # æ’å…¥åˆ°å†å²è®°å½•å¼€å¤´
else:
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ª CSV æ–‡ä»¶è¿›è¡ŒæŸ¥çœ‹ã€‚")

# æ˜¾ç¤ºä¸Šä¼ å†å²
st.subheader("æ–‡ä»¶ä¸Šä¼ å†å²")
if 'upload_history' in st.session_state:
    for idx, file_info in enumerate(st.session_state.upload_history):
        st.write(f"**æ–‡ä»¶ {idx + 1}**")
        st.write(f"æ–‡ä»¶åï¼š{file_info['æ–‡ä»¶å']}")
        st.write(f"ä¸Šä¼ æ—¶é—´ï¼š{file_info['ä¸Šä¼ æ—¶é—´']}")
        st.divider()
else:
    st.info("æš‚æ— å†å²è®°å½•")

import streamlit as st
import pandas as pd

# æƒé™éªŒè¯ï¼ˆæ‰€æœ‰å­é¡µé¢éƒ½éœ€è¦æ·»åŠ ï¼‰
if not st.session_state.get('logged_in'): 
    st.error(" è¯·å…ˆç™»å½•")
    st.stop()   # é˜»æ­¢ç»§ç»­åŠ è½½ 
    
# è·å–å·²ä¸Šä¼ çš„æ•°æ®
if 'data' in st.session_state:
    data = st.session_state.data
    st.write("### æ•°æ®å®æ—¶å±•ç¤ºï¼š")
    st.dataframe(data, use_container_width=True)
else:
    st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")

import streamlit as st 
import streamlit_echarts as st_echarts 
import numpy as np 
from sklearn.feature_extraction.text  import TfidfVectorizer 
from sklearn.decomposition  import LatentDirichletAllocation 
from sklearn.cluster  import KMeans 
from sklearn.manifold  import TSNE 
import jieba 
import pandas as pd 
 
# æƒé™éªŒè¯ 
if not st.session_state.get('logged_in'): 
    st.error(" è¯·å…ˆç™»å½•")
    st.stop() 
 
# åŠ è½½è‡ªå®šä¹‰è¯å…¸ 
jieba.load_userdict("custom_dict.txt") 
 
if 'data' in st.session_state: 
    data = st.session_state.data  
    st.write("###  ä¸»é¢˜å»ºæ¨¡åˆ†æç³»ç»Ÿ")
 
    # åŠ è½½ä¸­æ–‡åœç”¨è¯ 
    with open("stopwords.txt",  "r", encoding="utf-8") as f:
        stopwords = [line.strip() for line in f.readlines()] 
 
    # ä¸­æ–‡åˆ†è¯é¢„å¤„ç† 
    data['processed'] = data['review'].fillna('').apply(
        lambda x: " ".join([word for word in jieba.cut(x)  if word not in stopwords and len(word) > 1])
    )
 
    # ç‰¹å¾å‘é‡åŒ–å‡½æ•° 
    def vectorize_text(data, max_df=0.95, min_df=2):
        vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)
        X = vectorizer.fit_transform(data['processed']) 
        return X.astype(np.float32),  vectorizer.get_feature_names_out() 
 
    method = st.radio(" é€‰æ‹©åˆ†ææ–¹æ³•", ["LDAä¸»é¢˜æ¨¡å‹", "KMeansèšç±»åˆ†æ"])
 
    # é«˜çº§å‚æ•°é…ç½® 
    with st.expander(" é«˜çº§å‚æ•°é…ç½®"):
        max_df = st.slider(" æœ€å¤§æ–‡æ¡£é¢‘ç‡", 0.7, 1.0, 0.95)
        min_df = st.slider(" æœ€å°æ–‡æ¡£é¢‘ç‡", 1, 10, 2)
        use_tsne = st.checkbox(" å¯ç”¨t-SNEé™ç»´å¯è§†åŒ–(ä»…k-meansæ—¶å¯é€‰)")
        if method == "LDAä¸»é¢˜æ¨¡å‹":
            num_topics = st.slider(" ä¸»é¢˜æ•°é‡", 2, 10, 5)
        else:
            num_clusters = st.slider(" èšç±»æ•°é‡", 2, 10, 5)
        # ä¿®æ”¹åçš„LDAä¸»é¢˜å¯è§†åŒ–éƒ¨åˆ† 
    if method == "LDAä¸»é¢˜æ¨¡å‹":
        with st.spinner(' ä¸»é¢˜å»ºæ¨¡ä¸­...'):
            X, features = vectorize_text(data, max_df, min_df)
            lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
            lda.fit(X) 
    
            st.write("###  ä¸»é¢˜å…³é”®è¯åˆ†å¸ƒ")
            topic_options = []
            for idx, topic in enumerate(lda.components_): 
                top_indices = topic.argsort()[-10:][::-1] 
                top_words = [features[i] for i in top_indices]
                # å…³é”®ä¿®æ”¹ç‚¹ï¼šæ·»åŠ roundå‡½æ•°å¤„ç†å°æ•°ä½æ•° 
                weights = [round(float(topic[i]), 3) for i in top_indices]  # ä¸‰ä½å°æ•°å¤„ç† 
    
                # å½’ä¸€åŒ–å¤„ç† 
                weights = np.array(weights)  / np.sum(weights) 
                weights = [round(w, 3) for w in weights]  # å†æ¬¡å››èˆäº”å…¥
                
                option = {
                    "title": {"text": f"ä¸»é¢˜ {idx + 1}"},
                    "tooltip": {
                        "trigger": "axis",
                        "formatter": "{b}: {c}"  # æç¤ºæ¡†æ˜¾ç¤ºåŸå§‹å€¼ 
                    },
                    "xAxis": {"type": "value"},
                    "yAxis": {
                        "type": "category",
                        "data": top_words,
                        "axisLabel": {
                            "interval": 0,
                            "formatter": "{value}",
                            "fontSize": 12 
                        }
                    },
                    "series": [{
                        "type": "bar",
                        "data": weights,
                        "label": {
                            "show": True,
                            "position": 'right',
                            "formatter": "{@[1]}"  # æ˜¾ç¤ºä¸‰ä½å°æ•° 
                        }
                    }],
                    "grid": {
                        "left": "25%",
                        "right": "15%",  # å¢åŠ å³ä¾§ç©ºé—´ 
                        "bottom": "3%",
                        "containLabel": True 
                    }
                }
                topic_options.append(option) 
    
            # åˆ†åˆ—æ˜¾ç¤ºä¸»é¢˜ 
            cols = st.columns(2) 
            for idx, option in enumerate(topic_options):
                with cols[idx % 2]:
                    st_echarts.st_echarts( 
                        options=option,
                        height=f"{len(option['yAxis']['data'])*30 + 100}px",
                        key=f"topic{idx}"
                    )
 
    elif method == "KMeansèšç±»åˆ†æ":
        with st.spinner(' èšç±»åˆ†æä¸­...'):
            X, features = vectorize_text(data, max_df, min_df)
            kmeans = KMeans(n_clusters=num_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(X) 
 
            # èšç±»å…³é”®è¯å±•ç¤º 
            cluster_keywords = []
            for idx in range(num_clusters):
                cluster_words = [features[i] for i in kmeans.cluster_centers_[idx].argsort()[-10:][::-1]] 
                cluster_keywords.append(f"** èšç±» {idx + 1} å…³é”®è¯**ï¼š{', '.join(cluster_words)}")
 
            with st.expander(" æŸ¥çœ‹èšç±»å…³é”®è¯"):
                cols = st.columns(2) 
                for idx in range(num_clusters):
                    with cols[idx % 2]:
                        st.markdown(cluster_keywords[idx]) 
 
            # t-SNE å¯è§†åŒ– 
            if use_tsne and X.shape[0]  > 10:
                st.write("###  èšç±»åˆ†å¸ƒå¯è§†åŒ–")
                tsne = TSNE(n_components=2, random_state=42)
                embeddings = tsne.fit_transform(X.toarray()) 
 
                scatter_data = []
                for i in range(len(embeddings)):
                    scatter_data.append({ 
                        "value": [
                            float(embeddings[i, 0]),  # æ˜¾å¼è½¬æ¢ç±»å‹ 
                            float(embeddings[i, 1])
                        ],
                        "itemStyle": {"color": f"hsl({(cluster_labels[i] * 360) // num_clusters}, 60%, 60%)"}
                    })
 
                tsne_option = {
                    "title": {"text": "t-SNE èšç±»åˆ†å¸ƒå›¾"},
                    "xAxis": {"type": "value"},
                    "yAxis": {"type": "value"},
                    "series": [{
                        "type": "scatter",
                        "data": scatter_data,
                        "itemStyle": {"opacity": 0.6},
                        "symbolSize": 10 
                    }]
                }
                st_echarts.st_echarts(options=tsne_option,  height="600px")
else:
    st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚")

import streamlit as st
from wordcloud import WordCloud
import matplotlib.pyplot  as plt
import jieba
import jieba.analyse 

# ä½¿ç”¨ st.cache_resource  ç¼“å­˜ CampusWordFilter ç±»çš„å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
@st.cache_resource 
def get_campus_word_filter():
    return CampusWordFilter()

# ä½¿ç”¨ st.cache_data  ç¼“å­˜åŠ è½½åœç”¨è¯çš„æ“ä½œï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶
@st.cache_data 
def load_stopwords(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return set(f.read().splitlines()) 
    except FileNotFoundError:
        st.warning(f"   æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ {path}ï¼Œå¯ç”¨åŸºç¡€è¿‡æ»¤")
        return set(["çš„", "äº†", "æ˜¯", "åœ¨"])

# æƒé™éªŒè¯æ¨¡å—
def authenticate():
    if not st.session_state.get('logged_in'): 
        st.error("   è¯·å…ˆç™»å½•ç³»ç»Ÿ")
        st.stop() 

# é«˜æ ¡èˆ†æƒ…ä¸“ç”¨è¿‡æ»¤æ¨¡å—
class CampusWordFilter:
    def __init__(self):
        # æ ¸å¿ƒè¿‡æ»¤é…ç½®ï¼ˆå»ºè®®ä¿å­˜ä¸ºé…ç½®æ–‡ä»¶ï¼‰
        self.base_stopwords  = load_stopwords("stopwords.txt") 
        self.sensitive_words  = ["å¹¿å‘Š"]  # å¯æ‰©å±•
        self.education_keywords  = ["æ•™å­¦è´¨é‡", "è¯¾ç¨‹æ”¹é©", "ç§‘ç ”æˆæœ", "æ ¡å›­æ–‡åŒ–"]  # é‡ç‚¹ä¿ç•™è¯

    def filter_text(self, text):
        # ä½¿ç”¨TF-IDFæå–æ•™è‚²é¢†åŸŸå…³é”®è¯
        keywords = jieba.analyse.extract_tags(text, 
                                              topK=50,
                                              allowPOS=('n', 'ns', 'vn', 'nz'))

        # åŒé‡è¿‡æ»¤æœºåˆ¶
        words = [word for word in jieba.cut(text) 
                 if len(word) > 1
                 and word not in self.base_stopwords 
                 and word not in self.sensitive_words 
                 and word in keywords + self.education_keywords] 

        return " ".join(words)

# ä½¿ç”¨ st.cache_data  ç¼“å­˜ç”Ÿæˆè¯äº‘çš„æ“ä½œï¼Œé¿å…é‡å¤ç”Ÿæˆ
@st.cache_data 
def generate_campus_wordcloud(filtered_text, max_words):
    if not filtered_text:
        st.warning("   æœ‰æ•ˆæ–‡æœ¬å†…å®¹ä¸ºç©º")
        return None

    try:
        # å­—ä½“å…¼å®¹æ–¹æ¡ˆï¼ˆWindows/macOSï¼‰
        font_paths = [
            "C:/Windows/Fonts/msyh.ttc",   # Windows
            "/System/Library/Fonts/Supplemental/Songti.ttc"   # macOS
        ]

        for fp in font_paths:
            try:
                wc = WordCloud(
                    font_path=fp,
                    width=800,
                    height=400,
                    collocations=False,  # ç¦ç”¨è¯ç»„é‡å¤
                    background_color='white',
                    max_words=max_words
                ).generate(filtered_text)
                break
            except FileNotFoundError:
                continue
        else:
            st.warning("   æœªæ‰¾åˆ°ç³»ç»Ÿå­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
            wc = WordCloud().generate(filtered_text)

        # ç”Ÿæˆä¸“ä¸šå¯è§†åŒ–å›¾å½¢
        fig, ax = plt.subplots(figsize=(12,  6))
        ax.imshow(wc,  interpolation='bilinear')
        ax.axis('off') 
        plt.tight_layout() 
        return fig

    except Exception as e:
        st.error(f"   ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        return None

# ä¸»ç¨‹åº
def main():
    authenticate()

    # ç•Œé¢å¸ƒå±€ä¼˜åŒ–
    st.header("   è¯äº‘ç”Ÿæˆ")

    if 'data' not in st.session_state: 
        st.warning("   è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")
        return

    data = st.session_state.data 
    if data.empty: 
        st.error("   æ•°æ®é›†ä¸ºç©º")
        return

    with st.expander("ğŸ”§   é«˜çº§è®¾ç½®"):
        col1, col2 = st.columns(2) 
        with col1:
            min_word_length = st.radio( 
                "é€‰æ‹©æœ€å°è¯é•¿",
                options=[2, 3, 4],
                horizontal=True,  # æ°´å¹³æ’åˆ—
                index=0,
                format_func=lambda x: f"{x}å­—ç¬¦",
                help="è¿‡æ»¤ä½äºæ­¤é•¿åº¦çš„è¯è¯­"
            )
        with col2:
            max_words = st.slider( 
                "æœ€å¤§æ˜¾ç¤ºè¯æ•°",
                min_value=20, max_value=100, value=50,
                help="é™åˆ¶å±•ç¤ºç»“æœä¸­æœ€å¤šæ˜¾ç¤ºçš„è¯è¯­æ•°é‡"
            )

    # æ·»åŠ å¼€å§‹æŒ‰é’®
    if st.button("  å¼€å§‹"):
        # æ•°æ®é¢„å¤„ç†æµç¨‹
        try:
            processor = get_campus_word_filter()
            processed_text = processor.filter_text("    ".join(data['review'].dropna()))

            # if st.checkbox("   æ˜¾ç¤ºé¢„å¤„ç†æ–‡æœ¬"):
            #     st.code(processed_text[:1000]    + "...")

            # ç”Ÿæˆè¯äº‘
            if fig := generate_campus_wordcloud(processed_text, max_words):
                st.pyplot(fig) 

                # # æ‰©å±•åˆ†æåŠŸèƒ½
                # if st.button("   ç”Ÿæˆåˆ†ææŠ¥å‘Š"):
                #     generate_analysis_report(processed_text)

        except KeyError:
            st.error("   æ•°æ®é›†ä¸­ç¼ºå°‘'review'å­—æ®µ")

# def generate_analysis_report(text):
#     # å¯æ‰©å±•æ·»åŠ è¯é¢‘ç»Ÿè®¡ã€æƒ…æ„Ÿåˆ†æç­‰åŠŸèƒ½
#     st.success("   åˆ†ææŠ¥å‘Šç”Ÿæˆä¸­...")

if __name__ == "__main__":
    main()

import streamlit as st
import pandas as pd
import streamlit_echarts as ste
from snownlp import SnowNLP
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import io



# æ•°æ®å¤„ç†å‡½æ•°
@st.cache_data
def process_data(data):
    # åˆ é™¤å«æœ‰ NaN çš„è¡Œ
    data = data.dropna(subset=["review"])
    
    # ç­›é€‰è¯„è®ºä¸ºå­—ç¬¦ä¸²ç±»å‹çš„æ•°æ®
    data = data[data["review"].map(lambda x: isinstance(x, str))]
    
    # åˆ é™¤ç©ºå­—ç¬¦ä¸²å’Œæ— æ•ˆæ•°æ®
    data["review"] = data["review"].apply(lambda x: x.strip())
    data = data[data["review"] != ""]  # åˆ é™¤ç©ºå­—ç¬¦ä¸²
    
    return data

# æƒ…æ„Ÿåˆ†æå‡½æ•°
def calculate_sentiment(df):
    # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
    df["sentiment"] = df["review"].apply(handle_sentiment)
    
    # ä¿®é¥°æƒ…æ„Ÿåˆ†æ•°
    bins = [0, 0.4, 0.6, 1]
    labels = ["æ¶ˆæ", "ä¸­æ€§", "ç§¯æ"]
    df["sentiment_label"] = pd.cut(
        df["sentiment"], bins=bins, labels=labels, include_lowest=True
    )
    return df

def handle_sentiment(x):
    try:
        return SnowNLP(x).sentiments
    except:
        return 0.5  # è‡ªå®šä¹‰é»˜è®¤å€¼

# ç”Ÿæˆæƒ…æ„Ÿé¥¼å›¾
def build_pie_chart(df):
    sentiment_counts = (
        df["sentiment_label"].value_counts(normalize=True).mul(100).round(1)
    )

    # åŠ¨æ€ç”Ÿæˆæ•°æ®
    data = []
    for label in ["ç§¯æ", "ä¸­æ€§", "æ¶ˆæ"]:
        value = sentiment_counts.get(label, 0)
        item = {
            "value": value,
            "name": label,
            "itemStyle": {"color": "#ff0000" if label == "ç§¯æ" else "#9E9E9E" if label == "ä¸­æ€§" else "#0000ff"}
        }
        data.append(item)

    option = {
        "title": {
            "text": "èˆ†æƒ…æƒ…æ„Ÿåˆ†å¸ƒ",
            "left": "center",
            "textStyle": {"fontSize": 18},
        },
        "tooltip": {"trigger": "item"},
        "legend": {
            "orient": "vertical",
            "left": "left",
            "top": "middle",
        },
        "series": [
            {
                "type": "pie",
                "radius": ["40%", "70%"],
                "avoidLabelOverlap": True,
                "itemStyle": {
                    "borderRadius": 5,
                    "borderColor": "#fff",
                    "borderWidth": 2,
                },
                "label": {
                    "formatter": "{b}: {d}%",
                    "rich": {"fontSize": 14},
                },
                "emphasis": {
                    "itemStyle": {"shadowBlur": 10},
                    "label": {"show": True},
                },
                "data": data,
            }
        ],
    }
    return option

# ç”Ÿæˆæƒ…æ„Ÿè¶‹åŠ¿åˆ†æå›¾
def build_line_chart(df):
    # ç¡®ä¿ "å‘å¸ƒæ—¶é—´" åˆ—å­˜åœ¨
    if "å‘å¸ƒæ—¶é—´" not in df.columns:
        st.warning("æ•°æ®ä¸­æ²¡æœ‰å‘å¸ƒæ—¶é—´åˆ—ï¼Œæ— æ³•ç”Ÿæˆæƒ…æ„Ÿè¶‹åŠ¿å›¾ã€‚")
        return {}
    
    # è½¬æ¢æ—¶é—´ä¸ºæ—¥æœŸæ ¼å¼
    df["å‘å¸ƒæ—¶é—´"] = pd.to_datetime(df["å‘å¸ƒæ—¶é—´"]).dt.date  # Ensure date type
    
    # æŒ‰å¤©åˆ†ç»„è·å–æƒ…æ„Ÿå¾—åˆ†å‡å€¼å¹¶å¡«å……ç¼ºå¤±å€¼
    grouped = df.groupby("å‘å¸ƒæ—¶é—´")
    sentiment_trend = grouped["sentiment"].mean().reset_index()
    
    # åˆ›å»ºå®Œæ•´çš„æ—¥æœŸèŒƒå›´
    start_date = df["å‘å¸ƒæ—¶é—´"].min()
    end_date = df["å‘å¸ƒæ—¶é—´"].max()
    date_range = pd.date_range(start=start_date, end=end_date)
    
    # åˆ›å»ºæ–°çš„ DataFrame ç”¨äºåˆå¹¶
    sentiment_trend_display = pd.DataFrame({"æ—¶é—´": date_range.date})
    
    # åˆå¹¶å¹¶å¡«å……ç¼ºå¤±å€¼
    sentiment_trend_display = pd.merge(
        sentiment_trend_display, 
        sentiment_trend.rename(columns={"å‘å¸ƒæ—¶é—´": "æ—¶é—´"}),
        on="æ—¶é—´",
        how="left"
    ).fillna(0)  # å¡«å……ç©ºç¼ºå€¼
    
    # è°ƒæ•´å›¾è¡¨å±•ç¤ºçš„æ—¶é—´é—´éš”
    delta = (end_date - start_date).days
    if delta > 365:
        freq = "M"  # æ¯æœˆ
    elif delta > 30:
        freq = "W"  # æ¯å‘¨
    else:
        freq = "D"  # æ¯å¤©
    
    # åŠ¨æ€è°ƒæ•´ x è½´æ ‡ç­¾é—´éš”
    interval = get_dynamic_interval(delta)
    
    option = {
        "title": {
            "text": "èˆ†æƒ…æƒ…æ„Ÿè¶‹åŠ¿",
            "left": "center",
            "textStyle": {"fontSize": 18},
        },
        "tooltip": {"trigger": "axis"},
        "xAxis": {
            "type": "category",
            "data": sentiment_trend_display["æ—¶é—´"].astype(str).tolist(),
            "axisLabel": {
                "rotate": 45,
                "interval": interval,  # åŠ¨æ€é—´éš”
                "formatter": "{value}",
            },
            "boundaryGap": False,
        },
        "yAxis": {
            "type": "value",
            "name": "æƒ…æ„Ÿå¾—åˆ†",
            "min": 0,
            "max": 1,
        },
        "series": [
            {
                "name": "æƒ…æ„Ÿå¾—åˆ†",
                "type": "line",
                "data": sentiment_trend_display["sentiment"].tolist(),
                "symbolSize": 8,
                "smooth": True,
                "itemStyle": {"color": "#4CAF50"},
            }
        ],
    }
    return option

# åŠ¨æ€è®¡ç®— x è½´æ ‡ç­¾é—´éš”
def get_dynamic_interval(delta):
    if delta > 365:
        return 30  # æ¯æœˆæ˜¾ç¤ºä¸€æ¬¡
    elif delta > 30:
        return 7  # æ¯å‘¨æ˜¾ç¤ºä¸€æ¬¡
    elif delta > 14:
        return 3  # æ¯3å¤©æ˜¾ç¤ºä¸€æ¬¡
    else:
        return 1  # æ¯å¤©æ˜¾ç¤ºä¸€æ¬¡
    
def main():    
    # æƒé™éªŒè¯
    if not st.session_state.get("logged_in"):
        st.error("è¯·å…ˆç™»å½•")
        st.stop()

    # æ•°æ®ä¸Šä¼ ä¸åˆå§‹åŒ–
    if "data" not in st.session_state:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
        st.stop()
    else:
        data = st.session_state.data.copy()

    # æ•°æ®å¤„ç†ç®¡é“
    data = process_data(data)
    df = calculate_sentiment(data)

    # æ¸²æŸ“å›¾è¡¨
    st.write("### èˆ†æƒ…æƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–")
    pie_chart_options = build_pie_chart(df)
    ste.st_echarts(options=pie_chart_options, height="500px")

    st.write("### èˆ†æƒ…æƒ…æ„Ÿè¶‹åŠ¿åˆ†æ")
    line_chart_options = build_line_chart(df)
    ste.st_echarts(options=line_chart_options, height="500px")

if __name__ == "__main__":
    main()       

import streamlit as st
from pyecharts import options as opts
from pyecharts.charts import Map
import pandas as pd


def count_provinces(df):
    province_map = {
        "æ±Ÿè‹": "æ±Ÿè‹çœ",
        "æµ™æ±Ÿ": "æµ™æ±Ÿçœ",
        "å®‰å¾½": "å®‰å¾½çœ",
        "ç¦å»º": "ç¦å»ºçœ",
        "æ±Ÿè¥¿": "æ±Ÿè¥¿çœ",
        "å±±ä¸œ": "å±±ä¸œçœ",
        "æ²³å—": "æ²³å—çœ",
        "æ¹–åŒ—": "æ¹–åŒ—çœ",
        "æ¹–å—": "æ¹–å—çœ",
        "å¹¿ä¸œ": "å¹¿ä¸œçœ",
        "å¹¿è¥¿": "å¹¿è¥¿å£®æ—è‡ªæ²»åŒº",
        "æµ·å—": "æµ·å—çœ",
        "å››å·": "å››å·çœ",
        "è´µå·": "è´µå·çœ",
        "äº‘å—": "äº‘å—çœ",
        "è¥¿è—": "è¥¿è—è‡ªæ²»åŒº",
        "é™•è¥¿": "é™•è¥¿çœ",
        "ç”˜è‚ƒ": "ç”˜è‚ƒçœ",
        "é’æµ·": "é’æµ·çœ",
        "å®å¤": "å®å¤å›æ—è‡ªæ²»åŒº",
        "æ–°ç–†": "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº",
        "åŒ—äº¬": "åŒ—äº¬å¸‚",
        "å¤©æ´¥": "å¤©æ´¥å¸‚",
        "ä¸Šæµ·": "ä¸Šæµ·å¸‚",
        "é‡åº†": "é‡åº†å¸‚",
        "é¦™æ¸¯": "é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒº",
        "æ¾³é—¨": "æ¾³é—¨ç‰¹åˆ«è¡Œæ”¿åŒº",
        "å°æ¹¾": "å°æ¹¾çœ"
    }

    if 'ip' not in df.columns:
        st.error("CSV æ–‡ä»¶ä¸­æ²¡æœ‰åä¸º 'ip' çš„åˆ—ã€‚")
        return []
    
    # ä½¿ç”¨ .loc ç¡®ä¿æ“ä½œåœ¨åŸå§‹æ•°æ®æ¡†ä¸Šè¿›è¡Œ
    df.loc[df['ip'].notna(), 'ip'] = df.loc[df['ip'].notna(), 'ip'].map(province_map)
    
    # ç»Ÿè®¡çœä»½ IP å‡ºç°æ¬¡æ•°
    province_counts = df['ip'].value_counts().reset_index()
    province_counts.columns = ['çœä»½', 'æ¬¡æ•°']
    
    data = list(zip(province_counts['çœä»½'], province_counts['æ¬¡æ•°']))
    
    if not data:
        data = [["æœªè®°å½•", 0]]
    
    return data

# åˆ›å»ºåœ°å›¾
def create_china_map(data):
    c = Map()
    c.add("IPåˆ†å¸ƒ", data, "china")
    c.set_global_opts(
        title_opts=opts.TitleOpts(title="ä¸­å›½çœä»½ IP åˆ†å¸ƒåœ°å›¾"),
        visualmap_opts=opts.VisualMapOpts(max_=max([x[1] for x in data]) or 1)
    )
    c.set_series_opts(
        label_opts=opts.LabelOpts(
            is_show=True,
            color="blue",
            formatter="{b}: {c}"  # æ˜¾ç¤ºçœä»½åç§°å’Œæ¬¡æ•°
        )
    )
    return c

# ä¸»å‡½æ•°
def main():
    # æƒé™éªŒè¯ï¼ˆæ‰€æœ‰å­é¡µé¢éƒ½éœ€è¦æ·»åŠ ï¼‰
    if not st.session_state.get('logged_in'):
        st.error("è¯·å…ˆç™»å½•")
        st.stop()  # é˜»æ­¢ç»§ç»­åŠ è½½
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸Šä¼ çš„æ•°æ®
    if 'data' not in st.session_state:
        st.warning("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
        st.stop()  # åœæ­¢é¡µé¢åŠ è½½
    
    # è·å–å·²ä¸Šä¼ çš„æ•°æ®
    data = st.session_state.data
    
    # ç»Ÿè®¡çœä»½ IP æ•°æ®
    province_data = count_provinces(data)
    
    # åˆ›å»ºåœ°å›¾
    map_chart = create_china_map(province_data)
    
    # æ˜¾ç¤ºåœ°å›¾
    st.title("ä¸­å›½çœä»½ IP åˆ†å¸ƒåœ°å›¾")
    st.info("æ¨èä½¿ç”¨å¤¸å…‹æµè§ˆå™¨ï¼Œå…¶ä»–æµè§ˆå™¨æ‰“å¼€å¯èƒ½å­˜åœ¨æ˜¾ç¤ºé—®é¢˜")
    st.components.v1.html(map_chart.render_embed(), height=800)

if __name__ == "__main__":
    main()

import streamlit as st 
from pyecharts import options as opts 
from pyecharts.charts   import Bar, Pie 
import pandas as pd 
 
# é…ç½®ä¸­æ–‡ç¯å¢ƒ 
def set_chinese(): 
    return opts.InitOpts( 
        theme="light",  # å†…ç½®ä¸»é¢˜æ”¯æŒä¸­æ–‡ 
        animation_opts=opts.AnimationOpts(animation_threshold=2000), 
        width="100%", 
        height="600px" 
    ) 
 
# ç”ŸæˆæŸ±çŠ¶å›¾ 
def create_bar(data) -> Bar: 
    provinces = data.index.tolist()  
    counts = data.values.tolist()  
    
    bar = ( 
        Bar(init_opts=set_chinese()) 
       .add_xaxis(provinces) 
       .add_yaxis("èˆ†æƒ…æ•°é‡", counts) 
       .set_global_opts( 
            title_opts=opts.TitleOpts(title="å„åœ°èˆ†æƒ…æ•°é‡åˆ†å¸ƒ"), 
            xaxis_opts=opts.AxisOpts( 
                name="çœä»½", 
                axislabel_opts=opts.LabelOpts(rotate=45)  # å€¾æ–œæ ‡ç­¾ 
            ), 
            datazoom_opts=[opts.DataZoomOpts()]  # æ·»åŠ æ»šåŠ¨æ¡ 
        ) 
    ) 
    return bar 
 
# ç”Ÿæˆé¥¼å›¾ 
def create_pie(data) -> Pie: 
    pie = ( 
        Pie(init_opts=set_chinese()) 
       .add( 
            "", 
            [list(z) for z in zip(data.index.tolist(),   data.values.tolist())],  
            radius=["30%", "75%"],  # ç¯å½¢é¥¼å›¾ 
            rosetype="radius"  # å—ä¸æ ¼å°”ç«ç‘°å›¾ 
        ) 
       .set_global_opts( 
            title_opts=opts.TitleOpts(title="èˆ†æƒ…å„åœ°å æ¯”"), 
            legend_opts=opts.LegendOpts(orient="vertical", pos_top="15%", pos_left="2%") 
        ) 
       .set_series_opts(label_opts=opts.LabelOpts(formatter="{{b}}: {{d}}%"))  # ç™¾åˆ†æ¯”æ˜¾ç¤º 
    ) 
    return pie 
 
# ä¸»ç¨‹åº 
def main(): 
    # æƒé™éªŒè¯ï¼ˆæ‰€æœ‰å­é¡µé¢éƒ½éœ€è¦æ·»åŠ ï¼‰ 
    if not st.session_state.get('logged_in'):  
        st.error("  è¯·å…ˆç™»å½•") 
        st.stop()    # é˜»æ­¢ç»§ç»­åŠ è½½ 
 
    if 'data' in st.session_state:  
        data = st.session_state.data  
        st.title("  èˆ†æƒ…åœ°åŸŸåˆ†å¸ƒ") 
        data['ip'] = data['ip'].fillna('æœªçŸ¥') 
        province_counts = data['ip'].value_counts() 
 
        # ç§»é™¤åŒåˆ—å¸ƒå±€ 
        st.markdown("###   èˆ†æƒ…åœ°åŒºåˆ†å¸ƒæŸ±çŠ¶å›¾") 
        st_pyecharts(create_bar(province_counts), height=500) 
 
        st.markdown("###   èˆ†æƒ…åœ°åŒºå æ¯”é¥¼å›¾") 
        st_pyecharts(create_pie(province_counts), height=500) 
    
    else: 
        st.warning(" è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚") 
 
if __name__ == "__main__": 
    from streamlit_echarts import st_pyecharts  # éœ€è¦å®‰è£…è¯¥æ‰©å±• 
    main() 