import streamlit as st
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
import jieba.analyse
from mk import check_permissions

# ä½¿ç”¨ st.cache_resource ç¼“å­˜ CampusWordFilter ç±»çš„å®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
@st.cache_resource 
def get_campus_word_filter():
    return CampusWordFilter()

# ä½¿ç”¨ st.cache_data ç¼“å­˜åŠ è½½åœç”¨è¯æ–‡ä»¶çš„æ“ä½œï¼Œé¿å…é‡å¤è¯»å–
@st.cache_data 
def load_stopwords(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return set(f.read().splitlines()) 
    except FileNotFoundError:
        st.warning(f"æœªæ‰¾åˆ°åœç”¨è¯æ–‡ä»¶ {path}ï¼Œå¯ç”¨åŸºç¡€è¿‡æ»¤")
        return set(["çš„", "äº†", "æ˜¯", "åœ¨"])

# é«˜æ ¡èˆ†æƒ…ä¸“ç”¨è¿‡æ»¤æ¨¡å—
class CampusWordFilter:
    def __init__(self):
        # æ ¸å¿ƒè¿‡æ»¤é…ç½®
        self.base_stopwords = load_stopwords("stopwords.txt") 
        self.sensitive_words = ["å¹¿å‘Š"]  # å¯æ‰©å±•
        
        # é‡ç‚¹ä¿ç•™è¯ä»æ–‡ä»¶ä¸­è¯»å–
        self.education_keywords = self.load_custom_keywords("custom_dict.txt") 
                                        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not self.education_keywords:  
            self.education_keywords = ["æ•™å­¦è´¨é‡", "è¯¾ç¨‹æ”¹é©", "ç§‘ç ”æˆæœ", "æ ¡å›­æ–‡åŒ–"]
            # æ˜¾ç¤ºæç¤ºä¿¡æ¯
            st.warning("æœªæ‰¾åˆ° `custom_dict.txt` æ–‡ä»¶ï¼Œå¯ç”¨é»˜è®¤çš„é‡ç‚¹ä¿ç•™è¯ã€‚")

    def load_custom_keywords(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                # å»é™¤ç©ºè¡Œå’Œç©ºç™½å­—ç¬¦
                return [line.strip() for line in f.read().splitlines() 
                        if line.strip()]
        except FileNotFoundError:
            return []

    def load_custom_keywords(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return [line.strip() for line in f.read().splitlines() 
                        if line.strip()]
        except FileNotFoundError:
            return []

    def filter_text(self, text):
        # ä½¿ç”¨TF-IDFæå–æ•™è‚²é¢†åŸŸå…³é”®è¯
        keywords = jieba.analyse.extract_tags(text, 
                                              topK=50,
                                              allowPOS=('n', 'ns', 'vn', 'nz'))

        # åŒé‡è¿‡æ»¤æœºåˆ¶
        words = [word for word in jieba.cut(text) 
                 if len(word) > 1
                 and word not in self.base_stopwords 
                 and word not in self.sensitive_words 
                 and word in keywords + self.education_keywords] 

        return " ".join(words)

# ä½¿ç”¨ st.cache_data ç¼“å­˜ç”Ÿæˆè¯äº‘çš„æ“ä½œï¼Œé¿å…é‡å¤ç”Ÿæˆ
@st.cache_data 
def generate_campus_wordcloud(filtered_text, max_words):
    if not filtered_text:
        st.warning("æœ‰æ•ˆæ–‡æœ¬å†…å®¹ä¸ºç©º")
        return None

    try:
        # å­—ä½“å…¼å®¹æ–¹æ¡ˆï¼ˆWindows/macOSï¼‰
        font_paths = [
            "C:/Windows/Fonts/msyh.ttc",   # Windows
            "/System/Library/Fonts/Supplemental/Songti.ttc"   # macOS
        ]

        for fp in font_paths:
            try:
                wc = WordCloud(
                    font_path=fp,
                    width=800,
                    height=400,
                    collocations=False,  # ç¦ç”¨è¯ç»„é‡å¤
                    background_color='white',
                    max_words=max_words
                ).generate(filtered_text)
                break
            except FileNotFoundError:
                continue
        else:
            st.warning("æœªæ‰¾åˆ°ç³»ç»Ÿå­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
            wc = WordCloud().generate(filtered_text)

        # ç”Ÿæˆä¸“ä¸šå¯è§†åŒ–å›¾å½¢
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off') 
        plt.tight_layout() 
        return fig

    except Exception as e:
        st.error(f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        return None

# ä¸»ç¨‹åº
def main():
    check_permissions()

    # ç•Œé¢å¸ƒå±€ä¼˜åŒ–
    st.header("è¯äº‘ç”Ÿæˆ")

    if 'data' not in st.session_state: 
        st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")
        st.stop()
        return

    data = st.session_state.data 
    if data.empty: 
        st.error("æ•°æ®é›†ä¸ºç©º")
        return

    with st.expander("ğŸ”§ é«˜çº§è®¾ç½®"):
        col1, col2 = st.columns(2) 
        with col1:
            min_word_length = st.radio( 
                "é€‰æ‹©æœ€å°è¯é•¿",
                options=[2, 3, 4],
                horizontal=True,  # æ°´å¹³æ’åˆ—
                index=0,
                format_func=lambda x: f"{x}å­—ç¬¦",
                help="è¿‡æ»¤ä½äºæ­¤é•¿åº¦çš„è¯è¯­"
            )
        with col2:
            max_words = st.slider( 
                "æœ€å¤§æ˜¾ç¤ºè¯æ•°",
                min_value=20, max_value=100, value=50,
                help="é™åˆ¶å±•ç¤ºç»“æœä¸­æœ€å¤šæ˜¾ç¤ºçš„è¯è¯­æ•°é‡"
            )

    # æ·»åŠ å¼€å§‹æŒ‰é’®
    if st.button("å¼€å§‹"):
        # æ˜¾ç¤ºåŠ è½½åŠ¨ç”»
        with st.spinner("æ­£åœ¨ç”Ÿæˆè¯äº‘é›†é”¦å›¾ï¼Œè¯·ç¨ç­‰..."):
            try:
                processor = get_campus_word_filter()
                processed_text = processor.filter_text(" ".join(data['review'].dropna()))

                # ç”Ÿæˆè¯äº‘å¹¶æ˜¾ç¤º
                wordcloud_fig = generate_campus_wordcloud(processed_text, max_words)

                if wordcloud_fig:
                    st.pyplot(wordcloud_fig)

            except KeyError:
                st.error("æ•°æ®é›†ä¸­ç¼ºå°‘'review'å­—æ®µ")

if __name__ == "__main__":
    main()